{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TweetBERTTail(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pooler = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 768)\n",
    "            #torch.nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pooler(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class PredictionModel(torch.nn.Module):\n",
    "    def __init__(self, model_path: str = None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "        self.embeding_model = BertModel.from_pretrained(\"distilbert-base-cased\").to(device)\n",
    "        self.tail = TweetBERTTail().to(device)\n",
    "\n",
    "        if model_path is not None:\n",
    "            self.tail.load_state_dict(model_path)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.tokenizer(x, return_tensors=\"pt\").to(device)\n",
    "        embeddings = self.embeding_model(**tokens).pooler_output\n",
    "        embeddings = embeddings.squeeze()\n",
    "        return self.tail(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertModel: ['distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = PredictionModel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-5.1753e-02,  8.6723e-03,  1.8457e-02,  1.0705e-01,  4.7387e-02,\n        -1.2550e-02, -4.1381e-02,  1.7415e-02, -9.4474e-03,  2.6718e-02,\n        -6.1540e-02,  4.6875e-02,  1.2291e-02, -1.4739e-02,  6.5017e-03,\n        -3.3910e-02, -2.3421e-02,  3.3783e-03, -4.5979e-02,  9.6297e-02,\n         1.1235e-02, -6.0300e-02,  5.7197e-03, -5.0415e-02, -1.8855e-02,\n         2.5474e-03,  2.9027e-02, -1.5125e-02, -7.9652e-03,  2.8523e-02,\n         5.0679e-02, -3.9052e-02, -6.1008e-02,  1.8436e-02,  3.9764e-02,\n        -1.1704e-02, -3.7170e-02,  2.9829e-02, -2.1245e-02,  5.8740e-02,\n         2.2992e-02,  7.9015e-02, -9.8385e-05, -6.1546e-03,  1.1343e-02,\n         9.2477e-03, -5.4877e-02, -5.5621e-02,  1.7883e-02,  1.1044e-01,\n        -4.7329e-02, -1.3617e-02,  4.5889e-02,  4.9609e-02,  7.5569e-02,\n        -5.7605e-02,  5.4447e-03, -4.9616e-02, -7.9300e-03, -7.9615e-02,\n         1.0651e-02,  9.7496e-03,  1.8499e-02,  5.2637e-02,  4.3963e-03,\n         6.2632e-02,  3.2581e-02, -2.4365e-02,  7.0312e-02,  8.2099e-03,\n        -2.6336e-02,  6.7632e-02, -2.1495e-02,  3.0903e-02,  8.9257e-02,\n        -9.9741e-03,  7.5256e-02,  5.6269e-02,  4.2697e-02,  1.1948e-02,\n        -7.1188e-03, -1.2428e-02,  5.6445e-02,  7.0835e-03,  3.8303e-02,\n        -2.4574e-02,  5.4874e-02,  4.2111e-02,  1.5890e-02, -6.5608e-02,\n        -4.6959e-02,  1.4152e-02,  6.4906e-02,  3.2123e-02, -2.5411e-02,\n         8.0036e-03, -4.5458e-02, -3.9358e-02,  9.3220e-02,  5.8473e-02,\n         1.1573e-02,  3.2495e-02, -4.5941e-02,  6.3104e-03, -5.8734e-03,\n         5.9260e-02,  7.6719e-04,  5.3896e-02, -8.1423e-03,  6.8486e-02,\n        -6.2276e-03, -6.3611e-02, -8.6677e-02,  6.5750e-02,  4.9991e-02,\n        -2.7165e-02, -2.0819e-02, -4.5936e-02,  3.5619e-02,  1.1110e-01,\n        -9.2733e-02,  3.7179e-02,  3.0748e-02,  2.1645e-02, -7.0688e-03,\n        -5.2179e-03, -5.2115e-02,  5.6184e-02,  6.5224e-02,  2.7478e-02,\n        -5.2226e-02, -1.6004e-02, -4.3917e-02,  7.3621e-02, -4.7760e-02,\n         1.3147e-02,  3.1932e-02,  8.9096e-02,  5.5368e-02,  6.6705e-02,\n         3.6622e-02,  1.7106e-02,  1.1378e-02,  9.0745e-02,  2.0109e-02,\n        -1.0245e-02, -3.2080e-02, -5.9680e-03,  1.0854e-02, -1.2824e-02,\n        -1.5874e-02, -2.5514e-02, -3.3303e-02, -3.9921e-02,  1.7065e-02,\n        -2.1408e-03, -2.0266e-03,  5.3657e-02, -3.4642e-02,  6.3361e-02,\n         2.3498e-02,  3.6550e-02,  4.2116e-03, -2.5077e-02, -3.3407e-02,\n         5.4781e-02,  8.6753e-02,  1.0741e-02, -9.0221e-02,  2.2395e-02,\n         9.2039e-02, -1.1229e-02,  4.1712e-02, -4.0671e-02, -2.0540e-02,\n        -6.4267e-02,  5.9219e-03, -5.7000e-02, -5.3960e-03, -6.2344e-02,\n         1.6287e-02, -2.7271e-02, -1.4086e-02,  3.4600e-02,  4.7947e-02,\n        -4.7040e-02,  2.7291e-02, -2.0106e-02,  6.6842e-03,  1.6078e-03,\n         1.1684e-02,  2.7077e-02, -5.2810e-03, -5.9801e-02, -3.6658e-02,\n         4.2792e-02,  5.1444e-03,  6.5983e-02,  1.2564e-02,  9.0447e-03,\n        -4.5300e-02,  6.5589e-02,  3.2116e-02, -9.7919e-03, -2.7550e-02,\n        -1.5093e-02,  2.9191e-02, -1.4399e-02,  8.5195e-02, -1.0130e-02,\n         1.3837e-02,  9.5306e-02,  5.1786e-02,  3.3842e-02,  4.7721e-02,\n        -4.9683e-02,  3.4501e-02, -1.2853e-02, -1.5807e-02, -8.6333e-02,\n         7.7291e-02,  3.6184e-02, -5.9566e-02, -7.9090e-02,  1.7868e-02,\n        -9.6583e-03, -8.9564e-03,  3.6541e-02, -5.8070e-04,  3.3216e-02,\n         1.1955e-02,  6.7527e-03, -6.5583e-02,  4.9745e-02, -9.4843e-03,\n        -3.4484e-02, -4.7616e-02, -1.3079e-02, -1.0134e-02,  8.3871e-03,\n        -3.6733e-02,  3.6698e-02,  3.2189e-03,  2.2465e-02,  8.9935e-02,\n         3.9946e-02, -2.1769e-02, -3.1490e-02, -8.8085e-02, -6.9300e-02,\n         9.6771e-02,  7.2760e-02,  2.3404e-02, -1.6979e-02,  2.7700e-02,\n        -4.3567e-02, -4.7309e-02,  1.6487e-02, -1.6931e-02, -1.6007e-02,\n         2.5859e-02,  6.6613e-02, -4.9731e-02,  8.3705e-03,  6.2952e-03,\n        -2.3140e-02,  2.1653e-02,  2.9534e-02,  2.2737e-02, -2.8012e-03,\n         2.1314e-02, -5.7763e-02, -5.5566e-02, -7.0387e-02,  1.5564e-02,\n        -1.3620e-01, -7.2318e-03,  4.0396e-03,  1.7223e-02,  6.2695e-02,\n         8.5658e-03, -2.4807e-02,  4.2979e-02, -2.1053e-02,  7.8302e-03,\n         3.5849e-02, -7.4321e-03, -2.0191e-02, -5.0687e-02,  9.0218e-02,\n        -7.1762e-02,  4.5221e-02,  7.9498e-02, -4.4029e-02,  7.4666e-02,\n        -2.9540e-02, -4.4027e-03, -2.6210e-02,  2.4228e-02,  7.9746e-02,\n        -2.0565e-02, -7.3984e-02, -9.4834e-02,  7.4404e-02, -6.4075e-02,\n         3.3110e-02, -5.3412e-02, -1.9017e-02, -1.1632e-02,  4.6890e-02,\n        -6.2867e-02, -4.2864e-02, -1.3823e-01, -4.0358e-02, -3.8721e-02,\n        -4.1663e-02, -9.0143e-02, -8.5315e-02, -4.3654e-03,  3.7344e-03,\n        -2.9562e-04, -7.4135e-02, -3.4330e-03,  1.2840e-02, -7.9724e-03,\n         2.7610e-03,  2.6269e-02,  3.4870e-02, -5.8804e-02, -5.1374e-02,\n        -6.3612e-02, -1.1811e-02,  1.3275e-02, -3.1802e-02, -9.9521e-02,\n         6.2938e-02,  2.8166e-02, -2.1545e-02,  7.0625e-02, -4.0472e-02,\n        -4.6773e-02,  5.0336e-02,  3.4424e-02,  3.9465e-02,  5.3719e-02,\n        -1.3938e-02, -8.6101e-02, -2.0311e-03, -1.9765e-02,  3.8920e-03,\n        -1.1046e-01,  1.5262e-02, -5.3968e-03, -6.6220e-02,  1.2705e-02,\n        -3.4531e-02,  1.7793e-02, -8.1842e-02, -5.5725e-02, -2.1329e-02,\n         1.6673e-01,  6.3575e-02, -2.0830e-02,  1.4658e-02, -3.6750e-02,\n         1.0992e-02, -7.1675e-02, -1.8791e-03, -5.3624e-02,  8.5958e-02,\n         8.2974e-03, -5.8828e-02, -1.8022e-02, -5.1007e-02, -5.1526e-02,\n        -5.4961e-02,  2.1147e-02,  2.5064e-02, -8.0658e-02, -2.5577e-02,\n        -5.4672e-02, -1.2478e-02,  3.7543e-02, -1.1485e-03,  2.6987e-02,\n        -1.8637e-02, -2.9626e-02,  4.7122e-02,  7.1311e-03, -3.3360e-02,\n         9.8297e-04, -2.9820e-02, -2.0460e-02, -1.3115e-02,  2.8602e-02,\n         1.5933e-02,  8.8181e-02,  5.2188e-02, -7.0392e-02,  4.2231e-02,\n         1.2784e-03, -3.7303e-02, -2.4159e-02,  3.2107e-02, -2.1799e-02,\n         1.9893e-02,  4.1293e-02, -1.3484e-02,  6.3272e-02,  1.0817e-03,\n         1.5879e-02, -4.1392e-02, -7.4601e-02,  3.7862e-02, -3.3029e-02,\n        -3.2937e-02, -2.1176e-02,  6.5431e-02,  3.9119e-03,  1.7887e-03,\n        -1.2588e-02,  8.6729e-02, -4.4115e-04, -1.0146e-01, -1.0091e-02,\n         1.2793e-01, -9.3886e-03,  8.3746e-04,  7.7325e-02, -1.9843e-03,\n         4.7960e-02,  2.7931e-02,  2.1714e-02, -8.6303e-02,  4.4361e-03,\n         2.6444e-02,  1.9037e-02,  9.7166e-03,  4.5501e-02,  9.0351e-02,\n        -5.1713e-02, -3.9237e-02, -4.8961e-02, -5.3044e-02, -3.2787e-02,\n         1.9621e-02,  7.8150e-03,  5.7551e-03, -1.4835e-02, -5.0449e-02,\n         9.9494e-03, -3.9100e-02,  3.0936e-02,  9.4477e-02,  8.8357e-03,\n        -6.5744e-02,  4.9303e-02, -5.4263e-02,  1.1143e-02, -3.4417e-02,\n        -2.6171e-02, -5.9652e-03,  1.6211e-02, -1.0987e-01,  6.7315e-03,\n         2.9577e-02, -9.7792e-03, -3.9414e-02, -1.8791e-02,  1.7457e-02,\n         2.1565e-02,  3.7045e-02, -6.6785e-02, -4.0624e-02, -6.5898e-02,\n        -3.7903e-02, -6.1185e-02, -2.1442e-02, -2.8295e-02,  1.4880e-02,\n        -3.0428e-02,  8.5946e-02,  7.5570e-02,  2.3236e-02, -1.5301e-02,\n        -1.5385e-02, -5.3103e-02,  5.8415e-02, -7.4147e-02,  1.1843e-01,\n        -1.9700e-02, -2.4962e-02,  6.0678e-02,  6.6779e-02,  8.4495e-02,\n         2.0097e-02,  1.2893e-03, -4.3202e-02, -2.2754e-02, -1.4608e-02,\n        -1.3095e-02, -6.1810e-02,  7.4552e-03, -8.6723e-02, -3.0824e-02,\n        -9.8011e-03,  5.0856e-02, -7.9565e-02,  1.0887e-01, -2.1817e-02,\n         1.5714e-02,  1.1293e-02,  4.2174e-02, -4.9899e-02,  1.0434e-01,\n         2.2227e-02,  2.8342e-02,  3.4569e-02,  1.1648e-01, -1.3893e-02,\n        -6.7655e-02,  9.6510e-03,  5.2667e-02,  6.0707e-02, -4.0463e-03,\n        -2.9695e-02,  6.9849e-03, -7.3308e-03,  5.7045e-04, -4.3921e-02,\n        -9.2562e-02,  2.1667e-02, -6.7148e-02, -2.1801e-03, -1.0483e-02,\n         2.0854e-03, -3.5171e-03, -7.0574e-02,  1.3544e-01, -3.9929e-02,\n        -3.9426e-03, -5.8944e-02,  1.3453e-03,  1.9656e-02, -9.3877e-02,\n        -7.9003e-02,  4.2766e-03,  2.5204e-04, -1.3631e-02,  3.5129e-02,\n         1.1063e-01,  2.8840e-02, -1.4571e-02,  1.7981e-02,  1.3623e-02,\n        -1.5661e-03, -1.0386e-01, -5.6988e-02,  3.6317e-02, -1.2839e-02,\n         6.9354e-02, -1.8480e-02, -3.1373e-02,  1.0255e-02, -4.5651e-02,\n        -4.8067e-02,  8.3464e-02,  4.4777e-02,  9.1006e-03,  2.2185e-02,\n         3.8322e-02, -9.5485e-05,  5.2384e-02,  5.7665e-03,  7.1915e-02,\n        -1.0911e-01,  7.5084e-02,  2.0705e-02,  2.4370e-03,  4.8589e-02,\n         1.3350e-01,  4.3801e-02, -4.7785e-02,  4.5522e-02,  3.6062e-02,\n        -5.6602e-02,  1.7975e-02, -8.1726e-02, -4.3530e-02, -2.0560e-02,\n        -2.6700e-02,  5.5066e-02, -6.2733e-03, -5.8793e-02,  4.6061e-02,\n         9.0407e-03, -3.3798e-02,  1.9227e-02,  2.5457e-02, -3.1783e-02,\n         3.2101e-02, -3.3391e-02, -9.5406e-03, -1.3348e-01, -6.2267e-02,\n        -2.0012e-02, -2.0266e-03, -1.9556e-03,  6.0950e-02,  7.1292e-02,\n        -3.6273e-02,  9.7839e-02, -3.5844e-03, -7.8846e-03, -3.6827e-02,\n        -8.3094e-02,  6.3431e-02,  1.0362e-01, -2.7537e-02,  5.6778e-02,\n         2.2286e-02,  8.0040e-03,  1.0004e-02,  2.7698e-02, -3.2732e-02,\n        -6.4917e-02,  4.1419e-04, -2.6321e-02, -1.3660e-02,  5.9797e-02,\n        -1.1231e-02,  4.1243e-02,  1.0041e-01,  6.1745e-03, -4.6638e-03,\n         6.7404e-02, -1.0377e-02,  3.0716e-02,  7.9160e-02, -7.3983e-02,\n         4.8432e-02,  4.6837e-02,  1.1445e-03,  2.8043e-02,  2.9692e-03,\n        -2.6963e-02, -1.2337e-02,  2.1091e-02,  1.2382e-01,  4.5466e-02,\n         2.1283e-02,  1.8548e-02,  3.2366e-02,  7.9116e-02,  9.3533e-04,\n        -2.6669e-02,  2.0396e-02, -1.0963e-01,  2.4370e-02,  1.1937e-02,\n         4.9522e-02, -8.2484e-03,  7.5229e-02,  5.1007e-02, -5.0066e-02,\n         1.0585e-02, -1.7679e-02, -8.0821e-02, -2.3045e-02, -3.6178e-02,\n         2.1459e-02, -8.1923e-03, -6.5854e-02,  2.0209e-02,  4.4292e-04,\n         3.7444e-02, -8.3862e-03,  8.9594e-03,  3.6999e-02,  9.7044e-02,\n         7.9071e-04,  3.0149e-03,  6.7248e-02,  4.7001e-02,  2.7448e-02,\n         8.9748e-02, -8.6942e-03,  1.6696e-02, -3.0227e-02,  1.9721e-02,\n         2.4398e-02,  3.9044e-03,  5.8743e-03,  3.3161e-02, -2.1756e-03,\n         1.9380e-02,  9.3551e-03, -1.8257e-02, -6.9061e-02,  3.8944e-03,\n        -3.6268e-03,  8.9776e-03,  3.7034e-02, -2.4959e-02,  7.3204e-03,\n         7.8703e-02, -6.2545e-02,  6.9446e-02,  2.3466e-02,  1.4227e-02,\n        -1.2055e-02,  3.4362e-02, -3.9674e-02, -4.5706e-02,  7.9435e-02,\n         3.8960e-02, -2.6966e-02,  4.1383e-02,  6.9167e-03, -5.0595e-02,\n        -5.8823e-02,  7.4825e-03,  3.9782e-02, -5.3176e-02,  3.6925e-02,\n        -4.0651e-02,  5.4393e-03, -4.0575e-02,  2.9313e-02,  5.0140e-02,\n         4.0843e-02,  1.5559e-02,  1.5905e-02,  3.8590e-02, -6.2353e-02,\n        -5.4654e-03, -9.6905e-03, -6.6806e-02, -7.3583e-03,  5.2685e-02,\n         5.2528e-02,  3.3384e-02,  4.4070e-02, -5.3904e-02,  5.0289e-02,\n         1.9170e-02, -2.4722e-02, -7.1728e-03,  3.1426e-02,  3.6019e-02,\n        -1.6262e-02, -3.7626e-02, -9.2344e-03,  8.2915e-02, -1.4570e-03,\n         1.3525e-04,  3.1146e-02, -9.2806e-03,  3.2771e-02, -1.4153e-02,\n         3.0435e-02, -1.9341e-02, -2.9731e-02, -1.1816e-01,  1.5478e-02,\n        -6.7781e-03, -1.2371e-01,  5.4597e-02], device='cuda:0',\n       grad_fn=<AddBackward0>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"Marcin ma własne zdanie :)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}