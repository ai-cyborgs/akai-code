{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import transformers\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, BertModel, DistilBertModel\n",
    "from transformers import AutoModel, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from datasets import Dataset, ClassLabel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### PARAMS\n",
    "MAX_SAMPLES = 10000\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189815 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        short_description   category\n",
       "0       Health experts said it is too early to predict...  U.S. NEWS\n",
       "1       He was subdued by passengers and crew when he ...  U.S. NEWS\n",
       "2       \"Until you have a dog you don't understand wha...     COMEDY\n",
       "3       \"Accidentally put grown-up toothpaste on my to...  PARENTING\n",
       "4       Amy Cooper accused investment firm Franklin Te...  U.S. NEWS\n",
       "...                                                   ...        ...\n",
       "209522  Verizon Wireless and AT&T are already promotin...       TECH\n",
       "209523  Afterward, Azarenka, more effusive with the pr...     SPORTS\n",
       "209524  Leading up to Super Bowl XLVI, the most talked...     SPORTS\n",
       "209525  CORRECTION: An earlier version of this story i...     SPORTS\n",
       "209526  The five-time all-star center tore into his te...     SPORTS\n",
       "\n",
       "[189815 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv('dataset/tweet_dataset.csv')\n",
    "dataset_df.dropna(inplace=True)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, y = dataset_df[['short_description']], dataset_df[['category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = undersampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krawc\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "oh_encoder = LabelEncoder()\n",
    "y_enc = oh_encoder.fit_transform(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 36246\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = {\"text\": X_res[\"short_description\"], \"labels\": y_enc.tolist()}\n",
    "data_df = Dataset.from_dict(data_df)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertModel: ['distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "embeding_model = BertModel.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1343e67659614bac9a1c8974ba91f84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36246 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "dataset_features = data_df.features.copy()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(examples[\"text\"], return_tensors=\"pt\")\n",
    "        embedings = embeding_model(**tokens).pooler_output\n",
    "        embedings = embedings.squeeze()\n",
    "\n",
    "    return {\"embedings\": embedings}\n",
    "\n",
    "dataset = data_df.map(tokenize_function)\n",
    "dataset.features['labels'] = ClassLabel(num_classes=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'embedings'],\n",
       "    num_rows: 11\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"text\"])\n",
    "dataset.set_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"dataset/embedings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['embedings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle().select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.1, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TweetDataset(TorchDataset):\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.input_ids = dataset['input_ids']\n",
    "        self.attention_mask = dataset['attention_mask']\n",
    "        self.dataset = dataset.remove_columns(\"labels\")\n",
    "        self.labels = dataset['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        #anchor = self.input_ids[item]\n",
    "        anchor = self.dataset[item]\n",
    "        anchor_class = self.labels[item]\n",
    "        #anchor_attention = self.attention_mask[item]\n",
    "\n",
    "\n",
    "        positive_indices = self.labels == anchor_class\n",
    "        positive_indices = positive_indices.nonzero()\n",
    "        positive_idx = positive_indices[torch.randint(high=len(positive_indices), size=(1, ))[0]]\n",
    "        #positive_example = self.input_ids[positive_idx].flatten()\n",
    "        #positive_attention = self.attention_mask[positive_idx]\n",
    "        positive_example = self.dataset[positive_idx]\n",
    "\n",
    "        negative_indices = self.labels != anchor_class\n",
    "        negative_indices = negative_indices.nonzero()\n",
    "        negative_idx = negative_indices[torch.randint(high=len(negative_indices), size=(1, ))[0]]\n",
    "        #negative_example = self.input_ids[negative_idx].flatten()\n",
    "        #negative_attention = self.attention_mask[negative_idx]\n",
    "        negative_example = self.dataset[negative_idx]\n",
    "\n",
    "        return anchor, positive_example, negative_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = TweetDataset(split_dataset['train'])\n",
    "test_ds = TweetDataset(split_dataset['test'])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TweetBERT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pooler = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768)\n",
    "        )\n",
    "        self.tahn = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(**x)\n",
    "        x = self.pooler(x[0][:, 0])\n",
    "        return self.tahn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TweetBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "loss = TripletMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konsz\\AppData\\Local\\Temp\\ipykernel_21068\\1568605475.py:1: NeptuneDeprecationWarning: `init` is deprecated, use `init_run` instead. We'll end support of it in `neptune-client==1.0.0`.\n",
      "  run = neptune.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/konradszewczyk/TweetBuble/e/BUBL-39\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1125 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"konradszewczyk/TweetBuble\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0MWIyOTA1ZS03ODc3LTQ5MzQtYjk0OS05ZjNjYzdiMDFjMDcifQ==\",\n",
    ")\n",
    "\n",
    "os.mkdir(os.path.join('models', run['sys/id'].fetch()))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss_log = []\n",
    "    for batch_idx, (anchor, positive_ex, negative_ex) in enumerate(tqdm(train_dl)):\n",
    "        #anchor = anchor.to(device=device)\n",
    "        anchor = {k: v.to(device) for k, v in anchor.items()}\n",
    "        archor_output = model(anchor)\n",
    "\n",
    "        #positive_ex = positive_ex.to(device=device)\n",
    "        positive_ex = {k: v[0].to(device) for k, v in positive_ex.items()}\n",
    "        positive_ex_output = model(positive_ex)\n",
    "\n",
    "        #negative_ex = negative_ex.to(device=device)\n",
    "        negative_ex = {k: v[0].to(device) for k, v in negative_ex.items()}\n",
    "        negative_ex_output = model(negative_ex)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss(archor_output, positive_ex_output, negative_ex_output)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_log.append(train_loss.detach().cpu())\n",
    "\n",
    "    train_loss = np.mean(train_loss_log)\n",
    "    run['train_loss'].log(train_loss)\n",
    "    print(\"Epoch {:02d} train: {:.5f}\".format(epoch, train_loss))\n",
    "\n",
    "    file_name = 'epoch-{:02d}.pt'.format(epoch)\n",
    "    PATH = os.path.join('models', run['sys/id'].fetch(), file_name)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (anchor, positive_ex, negative_ex) in enumerate(tqdm(test_dl)):\n",
    "            #anchor = anchor.to(device=device)\n",
    "            anchor = {k: v.to(device) for k, v in anchor.items()}\n",
    "            archor_output = model(anchor)\n",
    "\n",
    "            #positive_ex = positive_ex.to(device=device)\n",
    "            positive_ex = {k: v[0].to(device) for k, v in positive_ex.items()}\n",
    "            positive_ex_output = model(positive_ex)\n",
    "\n",
    "            #negative_ex = negative_ex.to(device=device)\n",
    "            negative_ex = {k: v[0].to(device) for k, v in negative_ex.items()}\n",
    "            negative_ex_output = model(negative_ex)\n",
    "\n",
    "            test_loss = loss(archor_output, positive_ex_output, negative_ex_output)\n",
    "\n",
    "            test_loss_log.append(test_loss.cpu())\n",
    "\n",
    "    test_loss = np.mean(test_loss_log)\n",
    "    run['test_loss'].log(test_loss)\n",
    "    print(\"Epoch {:02d} val: {:.5f}\".format(epoch, test_loss))\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "047f0be344ab69ad7384c6f2e32100c8c7862b5ef6b41d9f62e3e104913a9859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
