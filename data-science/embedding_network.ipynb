{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import transformers\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, BertModel, DistilBertModel\n",
    "from transformers import AutoModel, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from datasets import Dataset, ClassLabel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import neptune.new as neptune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "### PARAMS\n",
    "MAX_SAMPLES = 10000\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        short_description   category\n0       Health experts said it is too early to predict...  U.S. NEWS\n1       He was subdued by passengers and crew when he ...  U.S. NEWS\n2       \"Until you have a dog you don't understand wha...     COMEDY\n3       \"Accidentally put grown-up toothpaste on my to...  PARENTING\n4       Amy Cooper accused investment firm Franklin Te...  U.S. NEWS\n...                                                   ...        ...\n209522  Verizon Wireless and AT&T are already promotin...       TECH\n209523  Afterward, Azarenka, more effusive with the pr...     SPORTS\n209524  Leading up to Super Bowl XLVI, the most talked...     SPORTS\n209525  CORRECTION: An earlier version of this story i...     SPORTS\n209526  The five-time all-star center tore into his te...     SPORTS\n\n[189815 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>short_description</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Health experts said it is too early to predict...</td>\n      <td>U.S. NEWS</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He was subdued by passengers and crew when he ...</td>\n      <td>U.S. NEWS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"Until you have a dog you don't understand wha...</td>\n      <td>COMEDY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n      <td>PARENTING</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Amy Cooper accused investment firm Franklin Te...</td>\n      <td>U.S. NEWS</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>209522</th>\n      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n      <td>TECH</td>\n    </tr>\n    <tr>\n      <th>209523</th>\n      <td>Afterward, Azarenka, more effusive with the pr...</td>\n      <td>SPORTS</td>\n    </tr>\n    <tr>\n      <th>209524</th>\n      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n      <td>SPORTS</td>\n    </tr>\n    <tr>\n      <th>209525</th>\n      <td>CORRECTION: An earlier version of this story i...</td>\n      <td>SPORTS</td>\n    </tr>\n    <tr>\n      <th>209526</th>\n      <td>The five-time all-star center tore into his te...</td>\n      <td>SPORTS</td>\n    </tr>\n  </tbody>\n</table>\n<p>189815 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv('dataset/tweet_dataset.csv')\n",
    "dataset_df.dropna(inplace=True)\n",
    "dataset_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X, y = dataset_df[['short_description']], dataset_df[['category']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Undersampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = undersampler.fit_resample(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One-hot encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konsz\\OneDrive\\Pulpit\\Inne\\AKAI-Hack\\akai-code\\data-science\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "oh_encoder = LabelEncoder()\n",
    "y_enc = oh_encoder.fit_transform(y_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'labels'],\n    num_rows: 36246\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = {\"text\": X_res[\"short_description\"], \"labels\": y_enc.tolist()}\n",
    "data_df = Dataset.from_dict(data_df).shuffle()\n",
    "data_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_df.features['labels'] = ClassLabel(num_classes=42)\n",
    "split_dataset = data_df.train_test_split(test_size=0.1, stratify_by_column=\"labels\")\n",
    "data_df = split_dataset['test']\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertModel: ['distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'vocab_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "embeding_model = BertModel.from_pretrained(\"distilbert-base-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3625 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "301b56649a1e430ea1fac342e5a43e41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "dataset_features = data_df.features.copy()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(examples[\"text\"], return_tensors=\"pt\")\n",
    "        embedings = embeding_model(**tokens).pooler_output\n",
    "        embedings = embedings.squeeze()\n",
    "\n",
    "    return {\"embedings\": embedings}\n",
    "\n",
    "dataset = data_df.map(tokenize_function)\n",
    "dataset.features['labels'] = ClassLabel(num_classes=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['labels', 'embedings'],\n    num_rows: 3625\n})"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"text\"])\n",
    "dataset.set_format(\"torch\")\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "dataset_pd = pd.DataFrame(dataset['embedings'])\n",
    "dataset_pd[\"labels\"] = pd.DataFrame(dataset['labels'])\n",
    "dataset_pd.to_csv('dataset/embeddings_smol.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0     0.286959 -0.072739  0.753478  0.029065 -0.044117  0.796469 -0.080700   \n1     0.357129 -0.174773  0.603911  0.026667  0.094043  0.766938 -0.142196   \n2     0.395993 -0.010106  0.656082  0.130611 -0.051991  0.719536 -0.271542   \n3     0.334255 -0.037806  0.703386  0.064638  0.027087  0.798698 -0.184298   \n4     0.320405  0.077909  0.684145  0.079528  0.158576  0.772303 -0.010687   \n...        ...       ...       ...       ...       ...       ...       ...   \n3620  0.421578 -0.019629  0.656779  0.174148 -0.021393  0.840218 -0.100070   \n3621  0.368945 -0.013736  0.691660  0.126277  0.095280  0.758165 -0.146781   \n3622  0.265164 -0.116977  0.706452  0.182964  0.142396  0.793411  0.043281   \n3623  0.329796 -0.057874  0.734024  0.115979  0.116618  0.739115 -0.073479   \n3624  0.385728 -0.076705  0.629858  0.121919 -0.058572  0.815975 -0.084316   \n\n             7         8         9  ...       759       760       761  \\\n0    -0.116337  0.106141  0.664438  ... -0.081861 -0.296287  0.481009   \n1    -0.122094  0.022330  0.673558  ... -0.077524 -0.412776  0.496480   \n2     0.046218  0.007320  0.625045  ... -0.233596 -0.333096  0.483195   \n3    -0.088888  0.138465  0.644953  ... -0.089683 -0.295783  0.493706   \n4     0.017744  0.207142  0.698566  ... -0.090975 -0.369075  0.639004   \n...        ...       ...       ...  ...       ...       ...       ...   \n3620 -0.011857  0.167257  0.692656  ... -0.110019 -0.271366  0.487649   \n3621 -0.064218  0.114663  0.645248  ... -0.107440 -0.471480  0.559714   \n3622 -0.093050  0.318244  0.642137  ... -0.205144 -0.313186  0.530374   \n3623 -0.093619  0.288657  0.674363  ... -0.155023 -0.314374  0.598662   \n3624 -0.121392  0.097275  0.664709  ... -0.198016 -0.289547  0.484054   \n\n           762       763       764       765       766       767  labels  \n0    -0.632536 -0.181246 -0.031159  0.367673 -0.146828 -0.786743       9  \n1    -0.645392 -0.021252  0.059162  0.468731  0.179680 -0.866986       2  \n2    -0.732185 -0.191192  0.217195  0.477800 -0.053176 -0.807459      13  \n3    -0.596528 -0.298668  0.063497  0.299139 -0.043658 -0.803471      33  \n4    -0.637897 -0.158942 -0.108213  0.344758 -0.076982 -0.767144      12  \n...        ...       ...       ...       ...       ...       ...     ...  \n3620 -0.640063 -0.209200  0.021151  0.340553 -0.006638 -0.781998      26  \n3621 -0.732553 -0.126775 -0.059502  0.456765 -0.030017 -0.840898      11  \n3622 -0.650075 -0.190040  0.123436  0.518753 -0.017935 -0.817771      19  \n3623 -0.596325 -0.178441 -0.073083  0.377727  0.009060 -0.792576       0  \n3624 -0.601011 -0.260665  0.012940  0.367910 -0.072655 -0.796177      33  \n\n[3625 rows x 769 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.286959</td>\n      <td>-0.072739</td>\n      <td>0.753478</td>\n      <td>0.029065</td>\n      <td>-0.044117</td>\n      <td>0.796469</td>\n      <td>-0.080700</td>\n      <td>-0.116337</td>\n      <td>0.106141</td>\n      <td>0.664438</td>\n      <td>...</td>\n      <td>-0.081861</td>\n      <td>-0.296287</td>\n      <td>0.481009</td>\n      <td>-0.632536</td>\n      <td>-0.181246</td>\n      <td>-0.031159</td>\n      <td>0.367673</td>\n      <td>-0.146828</td>\n      <td>-0.786743</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.357129</td>\n      <td>-0.174773</td>\n      <td>0.603911</td>\n      <td>0.026667</td>\n      <td>0.094043</td>\n      <td>0.766938</td>\n      <td>-0.142196</td>\n      <td>-0.122094</td>\n      <td>0.022330</td>\n      <td>0.673558</td>\n      <td>...</td>\n      <td>-0.077524</td>\n      <td>-0.412776</td>\n      <td>0.496480</td>\n      <td>-0.645392</td>\n      <td>-0.021252</td>\n      <td>0.059162</td>\n      <td>0.468731</td>\n      <td>0.179680</td>\n      <td>-0.866986</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.395993</td>\n      <td>-0.010106</td>\n      <td>0.656082</td>\n      <td>0.130611</td>\n      <td>-0.051991</td>\n      <td>0.719536</td>\n      <td>-0.271542</td>\n      <td>0.046218</td>\n      <td>0.007320</td>\n      <td>0.625045</td>\n      <td>...</td>\n      <td>-0.233596</td>\n      <td>-0.333096</td>\n      <td>0.483195</td>\n      <td>-0.732185</td>\n      <td>-0.191192</td>\n      <td>0.217195</td>\n      <td>0.477800</td>\n      <td>-0.053176</td>\n      <td>-0.807459</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.334255</td>\n      <td>-0.037806</td>\n      <td>0.703386</td>\n      <td>0.064638</td>\n      <td>0.027087</td>\n      <td>0.798698</td>\n      <td>-0.184298</td>\n      <td>-0.088888</td>\n      <td>0.138465</td>\n      <td>0.644953</td>\n      <td>...</td>\n      <td>-0.089683</td>\n      <td>-0.295783</td>\n      <td>0.493706</td>\n      <td>-0.596528</td>\n      <td>-0.298668</td>\n      <td>0.063497</td>\n      <td>0.299139</td>\n      <td>-0.043658</td>\n      <td>-0.803471</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.320405</td>\n      <td>0.077909</td>\n      <td>0.684145</td>\n      <td>0.079528</td>\n      <td>0.158576</td>\n      <td>0.772303</td>\n      <td>-0.010687</td>\n      <td>0.017744</td>\n      <td>0.207142</td>\n      <td>0.698566</td>\n      <td>...</td>\n      <td>-0.090975</td>\n      <td>-0.369075</td>\n      <td>0.639004</td>\n      <td>-0.637897</td>\n      <td>-0.158942</td>\n      <td>-0.108213</td>\n      <td>0.344758</td>\n      <td>-0.076982</td>\n      <td>-0.767144</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3620</th>\n      <td>0.421578</td>\n      <td>-0.019629</td>\n      <td>0.656779</td>\n      <td>0.174148</td>\n      <td>-0.021393</td>\n      <td>0.840218</td>\n      <td>-0.100070</td>\n      <td>-0.011857</td>\n      <td>0.167257</td>\n      <td>0.692656</td>\n      <td>...</td>\n      <td>-0.110019</td>\n      <td>-0.271366</td>\n      <td>0.487649</td>\n      <td>-0.640063</td>\n      <td>-0.209200</td>\n      <td>0.021151</td>\n      <td>0.340553</td>\n      <td>-0.006638</td>\n      <td>-0.781998</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>3621</th>\n      <td>0.368945</td>\n      <td>-0.013736</td>\n      <td>0.691660</td>\n      <td>0.126277</td>\n      <td>0.095280</td>\n      <td>0.758165</td>\n      <td>-0.146781</td>\n      <td>-0.064218</td>\n      <td>0.114663</td>\n      <td>0.645248</td>\n      <td>...</td>\n      <td>-0.107440</td>\n      <td>-0.471480</td>\n      <td>0.559714</td>\n      <td>-0.732553</td>\n      <td>-0.126775</td>\n      <td>-0.059502</td>\n      <td>0.456765</td>\n      <td>-0.030017</td>\n      <td>-0.840898</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3622</th>\n      <td>0.265164</td>\n      <td>-0.116977</td>\n      <td>0.706452</td>\n      <td>0.182964</td>\n      <td>0.142396</td>\n      <td>0.793411</td>\n      <td>0.043281</td>\n      <td>-0.093050</td>\n      <td>0.318244</td>\n      <td>0.642137</td>\n      <td>...</td>\n      <td>-0.205144</td>\n      <td>-0.313186</td>\n      <td>0.530374</td>\n      <td>-0.650075</td>\n      <td>-0.190040</td>\n      <td>0.123436</td>\n      <td>0.518753</td>\n      <td>-0.017935</td>\n      <td>-0.817771</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>3623</th>\n      <td>0.329796</td>\n      <td>-0.057874</td>\n      <td>0.734024</td>\n      <td>0.115979</td>\n      <td>0.116618</td>\n      <td>0.739115</td>\n      <td>-0.073479</td>\n      <td>-0.093619</td>\n      <td>0.288657</td>\n      <td>0.674363</td>\n      <td>...</td>\n      <td>-0.155023</td>\n      <td>-0.314374</td>\n      <td>0.598662</td>\n      <td>-0.596325</td>\n      <td>-0.178441</td>\n      <td>-0.073083</td>\n      <td>0.377727</td>\n      <td>0.009060</td>\n      <td>-0.792576</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3624</th>\n      <td>0.385728</td>\n      <td>-0.076705</td>\n      <td>0.629858</td>\n      <td>0.121919</td>\n      <td>-0.058572</td>\n      <td>0.815975</td>\n      <td>-0.084316</td>\n      <td>-0.121392</td>\n      <td>0.097275</td>\n      <td>0.664709</td>\n      <td>...</td>\n      <td>-0.198016</td>\n      <td>-0.289547</td>\n      <td>0.484054</td>\n      <td>-0.601011</td>\n      <td>-0.260665</td>\n      <td>0.012940</td>\n      <td>0.367910</td>\n      <td>-0.072655</td>\n      <td>-0.796177</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n<p>3625 rows Ã— 769 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0     0.286959 -0.072739  0.753478  0.029065 -0.044117  0.796469 -0.080700   \n1     0.357129 -0.174773  0.603911  0.026667  0.094043  0.766938 -0.142196   \n2     0.395993 -0.010106  0.656082  0.130611 -0.051991  0.719536 -0.271542   \n3     0.334255 -0.037806  0.703386  0.064638  0.027087  0.798698 -0.184298   \n4     0.320405  0.077909  0.684145  0.079528  0.158576  0.772303 -0.010687   \n...        ...       ...       ...       ...       ...       ...       ...   \n3620       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n3621       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n3622       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n3623       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n3624       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n\n             7         8         9  ...       759       760       761  \\\n0    -0.116337  0.106141  0.664438  ... -0.081861 -0.296287  0.481009   \n1    -0.122094  0.022330  0.673558  ... -0.077524 -0.412776  0.496480   \n2     0.046218  0.007320  0.625045  ... -0.233596 -0.333096  0.483195   \n3    -0.088888  0.138465  0.644953  ... -0.089683 -0.295783  0.493706   \n4     0.017744  0.207142  0.698566  ... -0.090975 -0.369075  0.639004   \n...        ...       ...       ...  ...       ...       ...       ...   \n3620       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n3621       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n3622       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n3623       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n3624       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n\n           762       763       764       765       766       767  labels  \n0    -0.632536 -0.181246 -0.031159  0.367673 -0.146828 -0.786743     NaN  \n1    -0.645392 -0.021252  0.059162  0.468731  0.179680 -0.866986     NaN  \n2    -0.732185 -0.191192  0.217195  0.477800 -0.053176 -0.807459     NaN  \n3    -0.596528 -0.298668  0.063497  0.299139 -0.043658 -0.803471     NaN  \n4    -0.637897 -0.158942 -0.108213  0.344758 -0.076982 -0.767144     NaN  \n...        ...       ...       ...       ...       ...       ...     ...  \n3620       NaN       NaN       NaN       NaN       NaN       NaN    26.0  \n3621       NaN       NaN       NaN       NaN       NaN       NaN    11.0  \n3622       NaN       NaN       NaN       NaN       NaN       NaN    19.0  \n3623       NaN       NaN       NaN       NaN       NaN       NaN     0.0  \n3624       NaN       NaN       NaN       NaN       NaN       NaN    33.0  \n\n[7250 rows x 769 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.286959</td>\n      <td>-0.072739</td>\n      <td>0.753478</td>\n      <td>0.029065</td>\n      <td>-0.044117</td>\n      <td>0.796469</td>\n      <td>-0.080700</td>\n      <td>-0.116337</td>\n      <td>0.106141</td>\n      <td>0.664438</td>\n      <td>...</td>\n      <td>-0.081861</td>\n      <td>-0.296287</td>\n      <td>0.481009</td>\n      <td>-0.632536</td>\n      <td>-0.181246</td>\n      <td>-0.031159</td>\n      <td>0.367673</td>\n      <td>-0.146828</td>\n      <td>-0.786743</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.357129</td>\n      <td>-0.174773</td>\n      <td>0.603911</td>\n      <td>0.026667</td>\n      <td>0.094043</td>\n      <td>0.766938</td>\n      <td>-0.142196</td>\n      <td>-0.122094</td>\n      <td>0.022330</td>\n      <td>0.673558</td>\n      <td>...</td>\n      <td>-0.077524</td>\n      <td>-0.412776</td>\n      <td>0.496480</td>\n      <td>-0.645392</td>\n      <td>-0.021252</td>\n      <td>0.059162</td>\n      <td>0.468731</td>\n      <td>0.179680</td>\n      <td>-0.866986</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.395993</td>\n      <td>-0.010106</td>\n      <td>0.656082</td>\n      <td>0.130611</td>\n      <td>-0.051991</td>\n      <td>0.719536</td>\n      <td>-0.271542</td>\n      <td>0.046218</td>\n      <td>0.007320</td>\n      <td>0.625045</td>\n      <td>...</td>\n      <td>-0.233596</td>\n      <td>-0.333096</td>\n      <td>0.483195</td>\n      <td>-0.732185</td>\n      <td>-0.191192</td>\n      <td>0.217195</td>\n      <td>0.477800</td>\n      <td>-0.053176</td>\n      <td>-0.807459</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.334255</td>\n      <td>-0.037806</td>\n      <td>0.703386</td>\n      <td>0.064638</td>\n      <td>0.027087</td>\n      <td>0.798698</td>\n      <td>-0.184298</td>\n      <td>-0.088888</td>\n      <td>0.138465</td>\n      <td>0.644953</td>\n      <td>...</td>\n      <td>-0.089683</td>\n      <td>-0.295783</td>\n      <td>0.493706</td>\n      <td>-0.596528</td>\n      <td>-0.298668</td>\n      <td>0.063497</td>\n      <td>0.299139</td>\n      <td>-0.043658</td>\n      <td>-0.803471</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.320405</td>\n      <td>0.077909</td>\n      <td>0.684145</td>\n      <td>0.079528</td>\n      <td>0.158576</td>\n      <td>0.772303</td>\n      <td>-0.010687</td>\n      <td>0.017744</td>\n      <td>0.207142</td>\n      <td>0.698566</td>\n      <td>...</td>\n      <td>-0.090975</td>\n      <td>-0.369075</td>\n      <td>0.639004</td>\n      <td>-0.637897</td>\n      <td>-0.158942</td>\n      <td>-0.108213</td>\n      <td>0.344758</td>\n      <td>-0.076982</td>\n      <td>-0.767144</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3620</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>3621</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>3622</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3623</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3624</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>33.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7250 rows Ã— 769 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['embedings'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle().select(range(5000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.1, stratify_by_column=\"labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TweetDataset(TorchDataset):\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.input_ids = dataset['input_ids']\n",
    "        self.attention_mask = dataset['attention_mask']\n",
    "        self.dataset = dataset.remove_columns(\"labels\")\n",
    "        self.labels = dataset['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        #anchor = self.input_ids[item]\n",
    "        anchor = self.dataset[item]\n",
    "        anchor_class = self.labels[item]\n",
    "        #anchor_attention = self.attention_mask[item]\n",
    "\n",
    "\n",
    "        positive_indices = self.labels == anchor_class\n",
    "        positive_indices = positive_indices.nonzero()\n",
    "        positive_idx = positive_indices[torch.randint(high=len(positive_indices), size=(1, ))[0]]\n",
    "        #positive_example = self.input_ids[positive_idx].flatten()\n",
    "        #positive_attention = self.attention_mask[positive_idx]\n",
    "        positive_example = self.dataset[positive_idx]\n",
    "\n",
    "        negative_indices = self.labels != anchor_class\n",
    "        negative_indices = negative_indices.nonzero()\n",
    "        negative_idx = negative_indices[torch.randint(high=len(negative_indices), size=(1, ))[0]]\n",
    "        #negative_example = self.input_ids[negative_idx].flatten()\n",
    "        #negative_attention = self.attention_mask[negative_idx]\n",
    "        negative_example = self.dataset[negative_idx]\n",
    "\n",
    "        return anchor, positive_example, negative_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = TweetDataset(split_dataset['train'])\n",
    "test_ds = TweetDataset(split_dataset['test'])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TweetBERTTail(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pooler = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(768, 768, bias=True)\n",
    "        )\n",
    "        self.tahn = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pooler(x)\n",
    "        return self.tahn(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = TweetBERT()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "loss = TripletMarginLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = neptune.init(\n",
    "    project=\"konradszewczyk/TweetBuble\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0MWIyOTA1ZS03ODc3LTQ5MzQtYjk0OS05ZjNjYzdiMDFjMDcifQ==\",\n",
    ")\n",
    "\n",
    "os.mkdir(os.path.join('models', run['sys/id'].fetch()))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss_log = []\n",
    "    for batch_idx, (anchor, positive_ex, negative_ex) in enumerate(tqdm(train_dl)):\n",
    "        #anchor = anchor.to(device=device)\n",
    "        anchor = {k: v.to(device) for k, v in anchor.items()}\n",
    "        archor_output = model(anchor)\n",
    "\n",
    "        #positive_ex = positive_ex.to(device=device)\n",
    "        positive_ex = {k: v[0].to(device) for k, v in positive_ex.items()}\n",
    "        positive_ex_output = model(positive_ex)\n",
    "\n",
    "        #negative_ex = negative_ex.to(device=device)\n",
    "        negative_ex = {k: v[0].to(device) for k, v in negative_ex.items()}\n",
    "        negative_ex_output = model(negative_ex)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss(archor_output, positive_ex_output, negative_ex_output)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_log.append(train_loss.detach().cpu())\n",
    "\n",
    "    train_loss = np.mean(train_loss_log)\n",
    "    run['train_loss'].log(train_loss)\n",
    "    print(\"Epoch {:02d} train: {:.5f}\".format(epoch, train_loss))\n",
    "\n",
    "    file_name = 'epoch-{:02d}.pt'.format(epoch)\n",
    "    PATH = os.path.join('models', run['sys/id'].fetch(), file_name)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (anchor, positive_ex, negative_ex) in enumerate(tqdm(test_dl)):\n",
    "            #anchor = anchor.to(device=device)\n",
    "            anchor = {k: v.to(device) for k, v in anchor.items()}\n",
    "            archor_output = model(anchor)\n",
    "\n",
    "            #positive_ex = positive_ex.to(device=device)\n",
    "            positive_ex = {k: v[0].to(device) for k, v in positive_ex.items()}\n",
    "            positive_ex_output = model(positive_ex)\n",
    "\n",
    "            #negative_ex = negative_ex.to(device=device)\n",
    "            negative_ex = {k: v[0].to(device) for k, v in negative_ex.items()}\n",
    "            negative_ex_output = model(negative_ex)\n",
    "\n",
    "            test_loss = loss(archor_output, positive_ex_output, negative_ex_output)\n",
    "\n",
    "            test_loss_log.append(test_loss.cpu())\n",
    "\n",
    "    test_loss = np.mean(test_loss_log)\n",
    "    run['test_loss'].log(test_loss)\n",
    "    print(\"Epoch {:02d} val: {:.5f}\".format(epoch, test_loss))\n",
    "\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "047f0be344ab69ad7384c6f2e32100c8c7862b5ef6b41d9f62e3e104913a9859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}