{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import transformers\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, BertModel, DistilBertModel\n",
    "from transformers import AutoModel, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from datasets import Dataset, ClassLabel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset/embeddings_smol.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0     0.286959 -0.072739  0.753478  0.029065 -0.044117  0.796469 -0.080700   \n1     0.357129 -0.174773  0.603911  0.026667  0.094043  0.766938 -0.142196   \n2     0.395993 -0.010106  0.656082  0.130611 -0.051991  0.719535 -0.271543   \n3     0.334255 -0.037806  0.703386  0.064638  0.027087  0.798698 -0.184298   \n4     0.320405  0.077909  0.684145  0.079528  0.158576  0.772303 -0.010687   \n...        ...       ...       ...       ...       ...       ...       ...   \n3620  0.421578 -0.019629  0.656779  0.174148 -0.021393  0.840218 -0.100070   \n3621  0.368945 -0.013736  0.691660  0.126277  0.095280  0.758165 -0.146781   \n3622  0.265164 -0.116977  0.706452  0.182964  0.142396  0.793411  0.043281   \n3623  0.329796 -0.057874  0.734024  0.115979  0.116618  0.739115 -0.073479   \n3624  0.385728 -0.076705  0.629858  0.121919 -0.058572  0.815975 -0.084316   \n\n             7         8         9  ...       759       760       761  \\\n0    -0.116337  0.106141  0.664438  ... -0.081861 -0.296287  0.481009   \n1    -0.122094  0.022330  0.673558  ... -0.077524 -0.412776  0.496480   \n2     0.046218  0.007320  0.625046  ... -0.233596 -0.333096  0.483195   \n3    -0.088888  0.138465  0.644953  ... -0.089683 -0.295783  0.493706   \n4     0.017744  0.207142  0.698566  ... -0.090975 -0.369075  0.639004   \n...        ...       ...       ...  ...       ...       ...       ...   \n3620 -0.011857  0.167257  0.692656  ... -0.110019 -0.271366  0.487649   \n3621 -0.064218  0.114663  0.645248  ... -0.107440 -0.471480  0.559715   \n3622 -0.093050  0.318244  0.642137  ... -0.205144 -0.313186  0.530374   \n3623 -0.093619  0.288657  0.674363  ... -0.155023 -0.314374  0.598662   \n3624 -0.121392  0.097275  0.664709  ... -0.198016 -0.289547  0.484054   \n\n           762       763       764       765       766       767  labels  \n0    -0.632536 -0.181246 -0.031159  0.367673 -0.146828 -0.786743       9  \n1    -0.645392 -0.021252  0.059162  0.468731  0.179680 -0.866986       2  \n2    -0.732185 -0.191192  0.217195  0.477800 -0.053176 -0.807459      13  \n3    -0.596528 -0.298668  0.063497  0.299139 -0.043658 -0.803471      33  \n4    -0.637897 -0.158942 -0.108213  0.344758 -0.076982 -0.767144      12  \n...        ...       ...       ...       ...       ...       ...     ...  \n3620 -0.640063 -0.209200  0.021151  0.340553 -0.006638 -0.781998      26  \n3621 -0.732553 -0.126775 -0.059502  0.456765 -0.030017 -0.840898      11  \n3622 -0.650075 -0.190040  0.123436  0.518753 -0.017935 -0.817771      19  \n3623 -0.596325 -0.178441 -0.073083  0.377727  0.009060 -0.792576       0  \n3624 -0.601011 -0.260665  0.012940  0.367910 -0.072655 -0.796177      33  \n\n[3625 rows x 769 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.286959</td>\n      <td>-0.072739</td>\n      <td>0.753478</td>\n      <td>0.029065</td>\n      <td>-0.044117</td>\n      <td>0.796469</td>\n      <td>-0.080700</td>\n      <td>-0.116337</td>\n      <td>0.106141</td>\n      <td>0.664438</td>\n      <td>...</td>\n      <td>-0.081861</td>\n      <td>-0.296287</td>\n      <td>0.481009</td>\n      <td>-0.632536</td>\n      <td>-0.181246</td>\n      <td>-0.031159</td>\n      <td>0.367673</td>\n      <td>-0.146828</td>\n      <td>-0.786743</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.357129</td>\n      <td>-0.174773</td>\n      <td>0.603911</td>\n      <td>0.026667</td>\n      <td>0.094043</td>\n      <td>0.766938</td>\n      <td>-0.142196</td>\n      <td>-0.122094</td>\n      <td>0.022330</td>\n      <td>0.673558</td>\n      <td>...</td>\n      <td>-0.077524</td>\n      <td>-0.412776</td>\n      <td>0.496480</td>\n      <td>-0.645392</td>\n      <td>-0.021252</td>\n      <td>0.059162</td>\n      <td>0.468731</td>\n      <td>0.179680</td>\n      <td>-0.866986</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.395993</td>\n      <td>-0.010106</td>\n      <td>0.656082</td>\n      <td>0.130611</td>\n      <td>-0.051991</td>\n      <td>0.719535</td>\n      <td>-0.271543</td>\n      <td>0.046218</td>\n      <td>0.007320</td>\n      <td>0.625046</td>\n      <td>...</td>\n      <td>-0.233596</td>\n      <td>-0.333096</td>\n      <td>0.483195</td>\n      <td>-0.732185</td>\n      <td>-0.191192</td>\n      <td>0.217195</td>\n      <td>0.477800</td>\n      <td>-0.053176</td>\n      <td>-0.807459</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.334255</td>\n      <td>-0.037806</td>\n      <td>0.703386</td>\n      <td>0.064638</td>\n      <td>0.027087</td>\n      <td>0.798698</td>\n      <td>-0.184298</td>\n      <td>-0.088888</td>\n      <td>0.138465</td>\n      <td>0.644953</td>\n      <td>...</td>\n      <td>-0.089683</td>\n      <td>-0.295783</td>\n      <td>0.493706</td>\n      <td>-0.596528</td>\n      <td>-0.298668</td>\n      <td>0.063497</td>\n      <td>0.299139</td>\n      <td>-0.043658</td>\n      <td>-0.803471</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.320405</td>\n      <td>0.077909</td>\n      <td>0.684145</td>\n      <td>0.079528</td>\n      <td>0.158576</td>\n      <td>0.772303</td>\n      <td>-0.010687</td>\n      <td>0.017744</td>\n      <td>0.207142</td>\n      <td>0.698566</td>\n      <td>...</td>\n      <td>-0.090975</td>\n      <td>-0.369075</td>\n      <td>0.639004</td>\n      <td>-0.637897</td>\n      <td>-0.158942</td>\n      <td>-0.108213</td>\n      <td>0.344758</td>\n      <td>-0.076982</td>\n      <td>-0.767144</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3620</th>\n      <td>0.421578</td>\n      <td>-0.019629</td>\n      <td>0.656779</td>\n      <td>0.174148</td>\n      <td>-0.021393</td>\n      <td>0.840218</td>\n      <td>-0.100070</td>\n      <td>-0.011857</td>\n      <td>0.167257</td>\n      <td>0.692656</td>\n      <td>...</td>\n      <td>-0.110019</td>\n      <td>-0.271366</td>\n      <td>0.487649</td>\n      <td>-0.640063</td>\n      <td>-0.209200</td>\n      <td>0.021151</td>\n      <td>0.340553</td>\n      <td>-0.006638</td>\n      <td>-0.781998</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>3621</th>\n      <td>0.368945</td>\n      <td>-0.013736</td>\n      <td>0.691660</td>\n      <td>0.126277</td>\n      <td>0.095280</td>\n      <td>0.758165</td>\n      <td>-0.146781</td>\n      <td>-0.064218</td>\n      <td>0.114663</td>\n      <td>0.645248</td>\n      <td>...</td>\n      <td>-0.107440</td>\n      <td>-0.471480</td>\n      <td>0.559715</td>\n      <td>-0.732553</td>\n      <td>-0.126775</td>\n      <td>-0.059502</td>\n      <td>0.456765</td>\n      <td>-0.030017</td>\n      <td>-0.840898</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3622</th>\n      <td>0.265164</td>\n      <td>-0.116977</td>\n      <td>0.706452</td>\n      <td>0.182964</td>\n      <td>0.142396</td>\n      <td>0.793411</td>\n      <td>0.043281</td>\n      <td>-0.093050</td>\n      <td>0.318244</td>\n      <td>0.642137</td>\n      <td>...</td>\n      <td>-0.205144</td>\n      <td>-0.313186</td>\n      <td>0.530374</td>\n      <td>-0.650075</td>\n      <td>-0.190040</td>\n      <td>0.123436</td>\n      <td>0.518753</td>\n      <td>-0.017935</td>\n      <td>-0.817771</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>3623</th>\n      <td>0.329796</td>\n      <td>-0.057874</td>\n      <td>0.734024</td>\n      <td>0.115979</td>\n      <td>0.116618</td>\n      <td>0.739115</td>\n      <td>-0.073479</td>\n      <td>-0.093619</td>\n      <td>0.288657</td>\n      <td>0.674363</td>\n      <td>...</td>\n      <td>-0.155023</td>\n      <td>-0.314374</td>\n      <td>0.598662</td>\n      <td>-0.596325</td>\n      <td>-0.178441</td>\n      <td>-0.073083</td>\n      <td>0.377727</td>\n      <td>0.009060</td>\n      <td>-0.792576</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3624</th>\n      <td>0.385728</td>\n      <td>-0.076705</td>\n      <td>0.629858</td>\n      <td>0.121919</td>\n      <td>-0.058572</td>\n      <td>0.815975</td>\n      <td>-0.084316</td>\n      <td>-0.121392</td>\n      <td>0.097275</td>\n      <td>0.664709</td>\n      <td>...</td>\n      <td>-0.198016</td>\n      <td>-0.289547</td>\n      <td>0.484054</td>\n      <td>-0.601011</td>\n      <td>-0.260665</td>\n      <td>0.012940</td>\n      <td>0.367910</td>\n      <td>-0.072655</td>\n      <td>-0.796177</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n<p>3625 rows Ã— 769 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class TweetDataset(TorchDataset):\n",
    "    def __init__(self, dataset: pd.DataFrame):\n",
    "        self.labels = torch.Tensor(dataset['labels'])\n",
    "        self.embeddings = torch.Tensor(dataset.drop(['labels'], axis=1).to_numpy())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        anchor = self.embeddings[item]\n",
    "        anchor_class = self.labels[item]\n",
    "\n",
    "        positive_indices = self.labels == anchor_class\n",
    "        positive_indices = positive_indices.nonzero()\n",
    "        positive_idx = positive_indices[torch.randint(high=len(positive_indices), size=(1, ))[0]]\n",
    "        #positive_example = self.input_ids[positive_idx].flatten()\n",
    "        #positive_attention = self.attention_mask[positive_idx]\n",
    "        positive_example = self.embeddings[positive_idx]\n",
    "\n",
    "        negative_indices = self.labels != anchor_class\n",
    "        negative_indices = negative_indices.nonzero()\n",
    "        negative_idx = negative_indices[torch.randint(high=len(negative_indices), size=(1, ))[0]]\n",
    "        #negative_example = self.input_ids[negative_idx].flatten()\n",
    "        #negative_attention = self.attention_mask[negative_idx]\n",
    "        negative_example = self.embeddings[negative_idx]\n",
    "\n",
    "        return anchor, positive_example, negative_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "dataset = TweetDataset(dataset_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3625, 768])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}